# Code architecture

Goals and objective of this repo:

- The library should be independent of llm providers
- It should support both text and audio modality. Audio is priority for the time being, text support will come later
- Objective is to test the tool call accuracy given the input.
- For testing the accuracy of audio models' tool calls, the flow should be like this:
  - I give a text prompt, based on that prompt it should generate audio, that audio will be generated by using a llm provider. There should be a way for the user to create the audio generating agent.
  - I pass that audio to a user's agent, either via an api call, or passing the LLM agent, then when I get the response from the mode, I should be able to verify if the tool call is correct or not.
